{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"text-align:center; \">\n",
        "<img src=\"https://intech.media/wp-content/uploads/2021/11/pixta_67679187_M.jpg\", style='width: 1000px; height: 600px;'>\n",
        "</p>\n",
        "\n",
        "\n",
        "This code is a deep learning workflow designed to train a text classification model using TensorFlow. It begins by importing necessary libraries such as NumPy, Pandas, TensorFlow, and others for data processing, model building, and evaluation. The workflow involves loading a dataset, vectorizing the text data using TensorFlow's `TextVectorization` layer, and splitting the data into training, validation, and test sets. A bidirectional LSTM model is built to handle the sequence-based nature of text, with additional layers like Dropout to prevent overfitting. The model is trained using a binary cross-entropy loss function, and various evaluation metrics such as Precision, Recall, and Accuracy are calculated. Finally, the model and its associated vectorizer configuration and weights are saved for future use. This process covers all the essential steps for a typical natural language processing task, from data preparation to model evaluation and saving."
      ],
      "metadata": {
        "id": "D5wttuHaAoeB"
      },
      "id": "D5wttuHaAoeB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Importing necessary libraries"
      ],
      "metadata": {
        "id": "_Frq_LqDHYMF"
      },
      "id": "_Frq_LqDHYMF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc8a672",
      "metadata": {
        "id": "0cc8a672"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Importing TensorFlow for deep learning tasks and checking GPU availability"
      ],
      "metadata": {
        "id": "q_Qeg4txCY4p"
      },
      "id": "q_Qeg4txCY4p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7omlkV6H4bB1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7omlkV6H4bB1",
        "outputId": "ccbf1263-8b83-4202-d92b-de6b4b73f418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Reading the CSV file containing the training dataset"
      ],
      "metadata": {
        "id": "6GqlXlHnCm9o"
      },
      "id": "6GqlXlHnCm9o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99a64cc2",
      "metadata": {
        "id": "99a64cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "70708bd9-6bcb-4974-883d-c9acbbceafa6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aed33bf985ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;31m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             handle = _BytesZipFile(\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# error: Incompatible types in assignment (expression has type \"ZipFile\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;31m# base class \"_BufferedWriter\" defined the type as \"BytesIO\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         self.buffer: zipfile.ZipFile = zipfile.ZipFile(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         )\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv.zip'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/train.csv.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Extracting features"
      ],
      "metadata": {
        "id": "vGdW5awqCwOa"
      },
      "id": "vGdW5awqCwOa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1db4e24",
      "metadata": {
        "id": "f1db4e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "8c62b041-7faf-4757-83ca-2bce6459a22a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ffffcf84961>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "X = df[\"comment_text\"]\n",
        "y = df[df.columns[2:]].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.TextVectorization"
      ],
      "metadata": {
        "id": "Tbhu1agkDC5j"
      },
      "id": "Tbhu1agkDC5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4537254",
      "metadata": {
        "id": "b4537254"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "MAX_FEATURES = 200000\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens = MAX_FEATURES,\n",
        "    output_sequence_length = 1800,\n",
        "    output_mode = \"int\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa124de",
      "metadata": {
        "id": "1aa124de"
      },
      "outputs": [],
      "source": [
        "vectorizer.adapt(X.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Vectorizing the input text data"
      ],
      "metadata": {
        "id": "9RZlYdYsDVcd"
      },
      "id": "9RZlYdYsDVcd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ec1a1a",
      "metadata": {
        "id": "d5ec1a1a"
      },
      "outputs": [],
      "source": [
        "vectorized_text = vectorizer(X.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Creating a TensorFlow dataset from the vectorized text and the labels"
      ],
      "metadata": {
        "id": "AgpRRgppDf6r"
      },
      "id": "AgpRRgppDf6r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5989b0f8",
      "metadata": {
        "id": "5989b0f8"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(160000)\n",
        "dataset = dataset.batch(32)\n",
        "dataset = dataset.prefetch(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Fetching one batch from the dataset"
      ],
      "metadata": {
        "id": "fbkJ4DuvDlv6"
      },
      "id": "fbkJ4DuvDlv6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684475c8",
      "metadata": {
        "id": "684475c8"
      },
      "outputs": [],
      "source": [
        "batch_X, batch_y = dataset.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Splitting the dataset into training, validation, and test sets"
      ],
      "metadata": {
        "id": "XI8NpSqoDrdo"
      },
      "id": "XI8NpSqoDrdo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b4f375",
      "metadata": {
        "id": "98b4f375"
      },
      "outputs": [],
      "source": [
        "train = dataset.take(int(len(dataset) * .7))\n",
        "val = dataset.skip(int(len(dataset) * .7)).take(int(len(dataset) * .2))\n",
        "test = dataset.skip(int(len(dataset) * .9)).take(int(len(dataset) * .1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Creating a generator for the training data"
      ],
      "metadata": {
        "id": "naVBQZ2WDxLU"
      },
      "id": "naVBQZ2WDxLU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815dc9e5",
      "metadata": {
        "id": "815dc9e5"
      },
      "outputs": [],
      "source": [
        "train_generator = train.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Importing necessary libraries"
      ],
      "metadata": {
        "id": "fsc1YGQVIFnb"
      },
      "id": "fsc1YGQVIFnb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5773d9",
      "metadata": {
        "id": "7f5773d9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding, TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.creating the model"
      ],
      "metadata": {
        "id": "nBcutrooD_6S"
      },
      "id": "nBcutrooD_6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ed00f7",
      "metadata": {
        "id": "53ed00f7"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_FEATURES + 1, 128))  # Embedding layer\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # LSTM layer\n",
        "model.add(Dropout(0.5))  # Dropout layer pour Ã©viter le sur-apprentissage\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.5))  # Dropout layer\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(6, activation=\"sigmoid\"))  # Multi-label classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Compiling with optimized loss function"
      ],
      "metadata": {
        "id": "D5S5VdHeEJC4"
      },
      "id": "D5S5VdHeEJC4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc9663b",
      "metadata": {
        "id": "7bc9663b"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"BinaryCrossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.Setting up callbacks: EarlyStopping to stop training early if validation loss stops improving, and ModelCheckpoint to save the best model"
      ],
      "metadata": {
        "id": "VQ9R2lEwEV2l"
      },
      "id": "VQ9R2lEwEV2l"
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(\"best_model.keras\", save_best_only=True)"
      ],
      "metadata": {
        "id": "ygikeXF__vw1"
      },
      "id": "ygikeXF__vw1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Training the model for 7 epochs using the training and validation datasets, with the defined callbacks"
      ],
      "metadata": {
        "id": "_wZCyU4AEfDR"
      },
      "id": "_wZCyU4AEfDR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f514347",
      "metadata": {
        "id": "3f514347",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train,\n",
        "    validation_data=val,\n",
        "    epochs=7,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Retrieving the history of the model's performance during training"
      ],
      "metadata": {
        "id": "27QW5MLVEofb"
      },
      "id": "27QW5MLVEofb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f77925",
      "metadata": {
        "id": "92f77925"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db2a0159",
      "metadata": {
        "id": "db2a0159"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Plotting the training and validation loss/accuracy over epochs"
      ],
      "metadata": {
        "id": "tlutxoOwEw13"
      },
      "id": "tlutxoOwEw13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d69f81",
      "metadata": {
        "id": "b0d69f81"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Vectorizing a sample input text and making a prediction with the trained model"
      ],
      "metadata": {
        "id": "yCSLtsyyE4A8"
      },
      "id": "yCSLtsyyE4A8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa54b02",
      "metadata": {
        "id": "bfa54b02"
      },
      "outputs": [],
      "source": [
        "input_text = vectorizer(\"I love you\")\n",
        "batch = test.as_numpy_iterator().next()\n",
        "res = model.predict(np.expand_dims(input_text,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.evaluation metrics"
      ],
      "metadata": {
        "id": "1Row2gFxFHJa"
      },
      "id": "1Row2gFxFHJa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe50002",
      "metadata": {
        "id": "abe50002"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = CategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Evaluating the model on the test set"
      ],
      "metadata": {
        "id": "EiPoqjwXFN1L"
      },
      "id": "EiPoqjwXFN1L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5f58de",
      "metadata": {
        "id": "fa5f58de"
      },
      "outputs": [],
      "source": [
        "for batch in test.as_numpy_iterator():\n",
        "    X_true, y_true = batch\n",
        "    yhat = model.predict(X_true)\n",
        "\n",
        "    y_true = y_true.flatten()\n",
        "    yhat = yhat.flatten()\n",
        "\n",
        "    pre.update_state(y_true, yhat)\n",
        "    re.update_state(y_true, yhat)\n",
        "    acc.update_state(y_true, yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Printing the evaluation results"
      ],
      "metadata": {
        "id": "SYzsPv3FFT_s"
      },
      "id": "SYzsPv3FFT_s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8857df",
      "metadata": {
        "id": "5e8857df"
      },
      "outputs": [],
      "source": [
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dvD7YywhIiSI",
      "metadata": {
        "id": "dvD7YywhIiSI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Saving the entire model in Keras format\n",
        "model.save(\"full_model.keras\")\n",
        "\n",
        "# Saving the configuration and weights of the TextVectorization layer\n",
        "vectorizer_config = vectorizer.get_config()\n",
        "vectorizer_weights = vectorizer.get_weights()\n",
        "\n",
        "with open(\"vectorizer_config.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer_config, f)\n",
        "\n",
        "with open(\"vectorizer_weights.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer_weights, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 44219,
          "sourceId": 8076,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2583.223899,
      "end_time": "2024-12-31T15:16:37.388071",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-31T14:33:34.164172",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}